{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "98d28237-0d16-4882-a3a2-ba94c6498112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "class SyntheticPPIBuilder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_bags: int = 1000,\n",
    "        seq_len_range: Tuple[int, int] = (150, 300),\n",
    "        domain_len_range: Tuple[int, int] = (20, 50),\n",
    "        num_domains_range: Tuple[int, int] = (2, 5),\n",
    "        positive_rate: float = 0.5,\n",
    "        num_motifs: int = 15,\n",
    "        seed: int = 42\n",
    "    ):\n",
    "        self.num_bags = num_bags\n",
    "        self.seq_len_range = seq_len_range\n",
    "        self.domain_len_range = domain_len_range\n",
    "        self.num_domains_range = num_domains_range\n",
    "        self.positive_rate = positive_rate\n",
    "        self.num_motifs = num_motifs\n",
    "        self.rng = random.Random(seed)\n",
    "        self.amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "        self.motifs = self._generate_motifs(self.num_motifs)\n",
    "        self.bags = []\n",
    "\n",
    "    def _generate_motifs(self, n: int) -> List[str]:\n",
    "        return [\n",
    "            ''.join(self.rng.choices(self.amino_acids, k=self.rng.randint(3, 5)))\n",
    "            for _ in range(n)\n",
    "        ]\n",
    "\n",
    "    def _generate_sequence(self) -> str:\n",
    "        length = self.rng.randint(*self.seq_len_range)\n",
    "        return ''.join(self.rng.choices(self.amino_acids, k=length))\n",
    "\n",
    "    def _sample_domains(self, seq: str) -> List[Tuple[int, int]]:\n",
    "        n_domains = self.rng.randint(*self.num_domains_range)\n",
    "        domains = []\n",
    "        attempts = 0\n",
    "        max_attempts = 20\n",
    "        while len(domains) < n_domains and attempts < max_attempts:\n",
    "            start = self.rng.randint(0, len(seq) - self.domain_len_range[1])\n",
    "            end = start + self.rng.randint(*self.domain_len_range)\n",
    "            if end <= len(seq):\n",
    "                if all(end <= s or start >= e for (s, e) in domains):  # No overlap\n",
    "                    domains.append((start, end))\n",
    "            attempts += 1\n",
    "        return domains\n",
    "\n",
    "    def _insert_motif(self, seq: str, start: int, motif: str) -> Tuple[str, Tuple[int, int]]:\n",
    "        \"\"\"Insert a motif at a domain location, updating the sequence and domain span.\"\"\"\n",
    "        new_seq = seq[:start] + motif + seq[start + len(motif):]\n",
    "        return new_seq, (start, start + len(motif))\n",
    "\n",
    "    def _create_bag(self, pid: int) -> Dict:\n",
    "        seqA = self._generate_sequence()\n",
    "        seqB = self._generate_sequence()\n",
    "        domA = self._sample_domains(seqA)\n",
    "        domB = self._sample_domains(seqB)\n",
    "\n",
    "        all_pairs = [(i, j) for i in range(len(domA)) for j in range(len(domB))]\n",
    "        has_positive = self.rng.random() < self.positive_rate\n",
    "        key_pair_count = self.rng.randint(1, min(3, len(all_pairs))) if has_positive else 0\n",
    "        key_pairs = self.rng.sample(all_pairs, k=key_pair_count) if key_pair_count > 0 else []\n",
    "\n",
    "        # Insert motifs for key pairs\n",
    "        for idx, (i, j) in enumerate(key_pairs):\n",
    "            motif = self.motifs[idx % len(self.motifs)]\n",
    "\n",
    "            # Update domain A with motif\n",
    "            startA, _ = domA[i]\n",
    "            seqA, new_domA = self._insert_motif(seqA, startA, motif)\n",
    "            domA[i] = new_domA\n",
    "\n",
    "            # Update domain B with motif\n",
    "            startB, _ = domB[j]\n",
    "            seqB, new_domB = self._insert_motif(seqB, startB, motif)\n",
    "            domB[j] = new_domB\n",
    "\n",
    "        instances = []\n",
    "        for i in range(len(domA)):\n",
    "            for j in range(len(domB)):\n",
    "                label = int((i, j) in key_pairs)\n",
    "                instances.append({\n",
    "                    \"domainA\": domA[i],\n",
    "                    \"domainB\": domB[j],\n",
    "                    \"label\": label\n",
    "                })\n",
    "\n",
    "        return {\n",
    "            \"proteinA\": f\"ProtA_{pid}\",\n",
    "            \"proteinB\": f\"ProtB_{pid}\",\n",
    "            \"sequenceA\": seqA,\n",
    "            \"sequenceB\": seqB,\n",
    "            \"domainsA\": domA,\n",
    "            \"domainsB\": domB,\n",
    "            \"instances\": instances\n",
    "        }\n",
    "\n",
    "    def build(self) -> List[Dict]:\n",
    "        self.bags = [self._create_bag(i) for i in range(self.num_bags)]\n",
    "        return self.bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "29a389c9-aa8d-47e4-9727-8c58f88dacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "class DomainEncoder:\n",
    "    def __init__(self, k: int = 3, vocab_size: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize the encoder.\n",
    "        Args:\n",
    "            k: k-mer size\n",
    "            vocab_size: number of most frequent k-mers to keep\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab: List[str] = []\n",
    "        self.fitted = False\n",
    "\n",
    "    def _get_kmers(self, sequence: str) -> List[str]:\n",
    "        return [sequence[i:i+self.k] for i in range(len(sequence) - self.k + 1)]\n",
    "\n",
    "    def fit(self, domain_sequences: List[str]):\n",
    "        \"\"\"Build k-mer vocabulary from a list of domain sequences.\"\"\"\n",
    "        all_kmers = []\n",
    "        for seq in domain_sequences:\n",
    "            all_kmers.extend(self._get_kmers(seq))\n",
    "        kmer_counts = Counter(all_kmers)\n",
    "        most_common = kmer_counts.most_common(self.vocab_size)\n",
    "        self.vocab = [k for k, _ in most_common]\n",
    "        self.fitted = True\n",
    "\n",
    "    def encode(self, sequence: str) -> np.ndarray:\n",
    "        \"\"\"Encode a domain sequence using the fitted k-mer vocabulary.\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"DomainEncoder must be fitted before encoding.\")\n",
    "        vec = np.zeros(len(self.vocab), dtype=np.float32)\n",
    "        kmers = self._get_kmers(sequence)\n",
    "        counts = Counter(kmers)\n",
    "        for i, kmer in enumerate(self.vocab):\n",
    "            vec[i] = counts[kmer]\n",
    "        if len(kmers) > 0:\n",
    "            vec /= len(kmers)  # normalize\n",
    "        return vec\n",
    "\n",
    "    def encode_many(self, sequences: List[str]) -> np.ndarray:\n",
    "        \"\"\"Batch encode a list of domain sequences.\"\"\"\n",
    "        return np.stack([self.encode(seq) for seq in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "12ef87d3-f202-4706-93d9-bbd9648ce3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def prepare_mil_data(\n",
    "    bags: List[Dict],\n",
    "    encoder: DomainEncoder,\n",
    ") -> Tuple[List[np.ndarray], List[int], List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Convert synthetic bags to MIL input format.\n",
    "\n",
    "    Returns:\n",
    "        X_bags: List of 2D arrays, each shape (num_instances, 2 * encoding_dim)\n",
    "        y_bags: List of bag labels (0/1)\n",
    "        key_instance_indices: List of lists of indices of positive instances per bag\n",
    "    \"\"\"\n",
    "    X_bags = []\n",
    "    y_bags = []\n",
    "    key_instance_indices = []\n",
    "\n",
    "    for bag in bags:\n",
    "        seqA = bag[\"sequenceA\"]\n",
    "        seqB = bag[\"sequenceB\"]\n",
    "        domainsA = bag[\"domainsA\"]\n",
    "        domainsB = bag[\"domainsB\"]\n",
    "        instances = bag[\"instances\"]\n",
    "\n",
    "        # Encode all domains once (cache)\n",
    "        encA = [encoder.encode(seqA[start:end]) for (start, end) in domainsA]\n",
    "        encB = [encoder.encode(seqB[start:end]) for (start, end) in domainsB]\n",
    "\n",
    "        instance_vectors = []\n",
    "        instance_labels = []\n",
    "        pos_indices = []\n",
    "\n",
    "        for idx, inst in enumerate(instances):\n",
    "            i, j = domainsA.index(inst[\"domainA\"]), domainsB.index(inst[\"domainB\"])\n",
    "            vecA = encA[i]\n",
    "            vecB = encB[j]\n",
    "            concat_vec = np.concatenate([vecA, vecB])\n",
    "            instance_vectors.append(concat_vec)\n",
    "\n",
    "            label = inst[\"label\"]\n",
    "            instance_labels.append(label)\n",
    "            if label == 1:\n",
    "                pos_indices.append(idx)\n",
    "\n",
    "        bag_label = 1 if any(instance_labels) else 0\n",
    "\n",
    "        X_bags.append(np.vstack(instance_vectors))\n",
    "        y_bags.append(bag_label)\n",
    "        key_instance_indices.append(pos_indices)\n",
    "\n",
    "    return X_bags, y_bags, key_instance_indices\n",
    "\n",
    "\n",
    "class OneHotDomainEncoder:\n",
    "    def __init__(self):\n",
    "        self.amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "        self.aa_to_idx = {aa: i for i, aa in enumerate(self.amino_acids)}\n",
    "        self.dim = len(self.amino_acids)\n",
    "\n",
    "    def one_hot(self, seq: str) -> np.ndarray:\n",
    "        mat = np.zeros((len(seq), self.dim), dtype=np.float32)\n",
    "        for i, aa in enumerate(seq):\n",
    "            if aa in self.aa_to_idx:\n",
    "                mat[i, self.aa_to_idx[aa]] = 1.0\n",
    "        return mat\n",
    "\n",
    "    def encode(self, seq: str, pooling: str = \"max\") -> np.ndarray:\n",
    "        oh = self.one_hot(seq)\n",
    "        if len(oh) == 0:\n",
    "            return np.zeros(self.dim)\n",
    "        if pooling == \"mean\":\n",
    "            return oh.mean(axis=0)\n",
    "        elif pooling == \"max\":\n",
    "            return oh.max(axis=0)\n",
    "        else:\n",
    "            raise ValueError(\"Pooling must be 'mean' or 'max'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dfe03a68-6829-451f-9be5-e9801643a406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['proteinA', 'proteinB', 'sequenceA', 'sequenceB', 'domainsA', 'domainsB', 'instances'])\n"
     ]
    }
   ],
   "source": [
    "builder = SyntheticPPIBuilder(num_bags=1000)\n",
    "bags = builder.build()\n",
    "print(bags[0].keys())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "121c6ee3-0fb8-4eaf-b532-0670373a0937",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_domains = [\n",
    "    bag[\"sequenceA\"][start:end]\n",
    "    for bag in bags\n",
    "    for (start, end) in bag[\"domainsA\"]\n",
    "] + [\n",
    "    bag[\"sequenceB\"][start:end]\n",
    "    for bag in bags\n",
    "    for (start, end) in bag[\"domainsB\"]\n",
    "]\n",
    "\n",
    "# encoder = DomainEncoder(k=3, vocab_size=1000)\n",
    "# encoder.fit(all_domains)\n",
    "\n",
    "\n",
    "encoder = OneHotDomainEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e87fe514-9dd2-42ad-9bd3-a09f63764730",
   "metadata": {},
   "outputs": [],
   "source": [
    "bags, labels, key_pos = prepare_mil_data(bags, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "166700d7-8216-46bc-a96f-1b1ba85c7570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 40)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bags[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39c793-0554-4ff0-9e0b-4dfab344db52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "84424b10-914f-44a2-aaa6-fc2319fa569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from qsarmil.mil.wrapper import InstanceWrapper, BagWrapper\n",
    "from qsarmil.mil.network.regressor import InstanceNetworkRegressor, BagNetworkRegressor\n",
    "\n",
    "from qsarmil.mil.network.classifier import (AttentionNetworkClassifier,\n",
    "                                            GatedAttentionNetworkClassifier,\n",
    "                                            SelfAttentionNetworkClassifier,\n",
    "                                            TempAttentionNetworkClassifier,\n",
    "                                            GaussianPoolingNetworkClassifier,\n",
    "                                            DynamicPoolingNetworkClassifier)\n",
    "\n",
    "from qsarmil.mil.preprocessing import BagMinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "51053568-2934-4b83-aca4-33a7753b3b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def kid_top1(\n",
    "    key_indices: List[List[int]],\n",
    "    predictions: List[List[float]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute top-1 key instance detection accuracy for multiple key instances per bag.\n",
    "\n",
    "    Args:\n",
    "        key_indices: List of lists of true key instance indices per bag.\n",
    "        predictions: List of predicted scores per instance for each bag.\n",
    "\n",
    "    Returns:\n",
    "        Accuracy: Fraction of bags where top predicted instance is among key instances.\n",
    "    \"\"\"\n",
    "    assert len(key_indices) == len(predictions)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for keys, scores in zip(key_indices, predictions):\n",
    "        if not keys:\n",
    "            continue  # skip bags without key instances (e.g., negative bags)\n",
    "        total += 1\n",
    "        top_idx = max(range(len(scores)), key=lambda i: scores[i])\n",
    "        if top_idx in keys:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "487cd244-66cf-4b35-82da-f01c8319ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_hparams = {'hidden_layer_sizes':(256, 128, 64),\n",
    "                   'num_epoch':300,\n",
    "                   'batch_size':128,\n",
    "                   'learning_rate':0.001,\n",
    "                   'weight_decay':0.001,\n",
    "                   'instance_weight_dropout':0.01,\n",
    "                   'init_cuda':False,\n",
    "                   'verbose':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d686d69e-64d2-4b73-a4a4-1d7501e1d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, key_train, key_test = train_test_split(bags, labels, key_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f887e5e6-8310-4478-835a-6bb53383b3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GatedAttentionNetworkClassifier(\n",
       "  (main_net): Sequential(\n",
       "    (0): Linear(in_features=40, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (attention_V): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (attention_U): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (detector): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (estimator): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GatedAttentionNetworkClassifier(**network_hparams)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ba7bca49-c2b0-4c91-b394-0138c104e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict(x_test)\n",
    "y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "w_pred = model.get_instance_weights(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "678c932d-bbc3-43f2-9a8f-59728571e50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, np.where(model.predict(x_train) > 0.5, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "18ef3b2f-4fd4-41a6-a908-fdacd4ec6a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "25cceca6-6647-4576-960b-80cffbb51d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9366197183098591"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kid_top1(key_test, w_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf10503-13c6-41d2-a540-725360ef8326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8dc61-0a55-4e26-9325-2adec7913eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "be4b758d-cf30-4ba6-bbad-98e19be6ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "43b877a6-ebb6-4609-b291-33d49ba8f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "[5, 6]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[N])\n",
    "print(y_pred[N].item())\n",
    "print([i for i in key_test[N]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "17c3f5f1-f180-4fb3-82ff-5f0192f7ed54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.04, 0.02, 0.01, 0.06, 0.47, 0.27, 0.02, 0.01, 0.05, 0.02,\n",
       "       0.01], dtype=float32)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_pred[N].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c26b5-061c-4172-97bc-23c8cca65791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bb64f8d2-7178-4411-a1e7-650463f33db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_list = [\n",
    "                (\"AttentionNetworkClassifier\", AttentionNetworkClassifier(**network_hparams)), \n",
    "                (\"GatedAttentionNetworkClassifier\", GatedAttentionNetworkClassifier(**network_hparams)), \n",
    "                (\"SelfAttentionNetworkClassifier\", SelfAttentionNetworkClassifier(**network_hparams)), \n",
    "                (\"TemperatureAttentionNetworkClassifier\", TempAttentionNetworkClassifier(**network_hparams)),\n",
    "                (\"GaussianPoolingNetworkClassifier\", GaussianPoolingNetworkClassifier(**network_hparams)), \n",
    "                (\"DynamicPoolingNetworkClassifier\", DynamicPoolingNetworkClassifier(**network_hparams))\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e1aa6758-68cf-4a6d-9006-80bef55f5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "for name, model in network_list:\n",
    "    # train model\n",
    "    model.fit(x_train, y_train)\n",
    "    # predict\n",
    "    y_prob = model.predict(x_test)\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    w_pred = model.get_instance_weights(x_test)\n",
    "    #\n",
    "    res.loc[name, \"PRED_ACC\"] = accuracy_score(y_test, y_pred)\n",
    "    res.loc[name, \"KID_ACC\"] = kid_top1(key_test, w_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eb1235b7-e834-405d-b5ed-1d9f395844f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRED_ACC</th>\n",
       "      <th>KID_ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AttentionNetworkClassifier</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GatedAttentionNetworkClassifier</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SelfAttentionNetworkClassifier</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TemperatureAttentionNetworkClassifier</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianPoolingNetworkClassifier</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DynamicPoolingNetworkClassifier</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       PRED_ACC  KID_ACC\n",
       "AttentionNetworkClassifier                 1.00     0.70\n",
       "GatedAttentionNetworkClassifier            1.00     0.94\n",
       "SelfAttentionNetworkClassifier             1.00     0.04\n",
       "TemperatureAttentionNetworkClassifier      1.00     0.87\n",
       "GaussianPoolingNetworkClassifier           0.57     0.62\n",
       "DynamicPoolingNetworkClassifier            1.00     0.94"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012739f-e6db-4826-a26a-9651cbdfb47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4364d603-af9c-449c-8787-a918db5f2b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb7e6b-fbd7-49b1-b92b-e3677ed95dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654096ed-c1ba-48f6-bc30-377de6bc83fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qsarmil",
   "language": "python",
   "name": "qsarmil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
