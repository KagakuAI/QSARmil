{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Instance Learning for Molecular Conformers\n",
    "\n",
    "In molecular modeling, a single molecule can adopt multiple **conformations** due to rotations around single bonds. Each conformation, or **conformer**, may exhibit slightly different physicochemical properties or biological activity.  \n",
    "\n",
    "Traditional QSAR or ML approaches often select a single “representative” conformer per molecule, which can **miss important structural information**. Multiple Instance Learning (MIL) offers a principled solution.\n",
    "\n",
    "---\n",
    "\n",
    "### What is MIL?\n",
    "\n",
    "**Multiple Instance Learning (MIL)** is a type of machine learning where **labels are associated with sets of instances (bags)** rather than individual instances.  \n",
    "- **Bag:** A molecule  \n",
    "- **Instance:** A conformer of that molecule  \n",
    "- **Label:** Molecular property or activity  \n",
    "\n",
    "The MIL model learns to predict the **bag-level label** while potentially considering the contributions of individual instances.  \n",
    "\n",
    "This framework naturally accommodates the **multi-conformer nature of molecules** and allows models to:\n",
    "1. Aggregate information across all conformers.\n",
    "2. Optionally identify **key conformers** that drive the molecular property.\n",
    "\n",
    "---\n",
    "\n",
    "### Goals of this Notebook\n",
    "\n",
    "1. Generate conformers for a set of molecules.  \n",
    "2. Compute 3D molecular descriptors for each conformer.  \n",
    "3. Train MIL models to predict molecular properties using all conformers.  \n",
    "4. (Optional) Identify key conformers that contribute most strongly to the predictions.  \n",
    "\n",
    "By the end of this tutorial, you will understand how MIL can **leverage conformational diversity** in molecules for predictive modeling and interpretable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset\n",
    "\n",
    "The example datasets contain molecule structure (SMILES) and measured bioactivity (pKi or IC50) – the higher the better. Each SMILES is converted to a Mol object in RDKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_to_clf(y):\n",
    "    return np.where(np.array(y) > 6, 1, 0)\n",
    "\n",
    "def accuracy_metric(y_true, y_pred, task=None):\n",
    "    if task == \"classification\":\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "    elif task == \"regression\":\n",
    "        return r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"regression\"\n",
    "# TASK = \"classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID = \"KagakuData/notebooks\"\n",
    "\n",
    "csv_path = hf_hub_download(REPO_ID, filename=\"chembl/CHEMBL279.csv\", repo_type=\"dataset\")\n",
    "data = pd.read_csv(csv_path, header=None)\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "smi_train, prop_train = data_train[0].to_list(), data_train[2].to_list()\n",
    "smi_test, prop_test = data_test[0].to_list(), data_test[2].to_list()\n",
    "\n",
    "if TASK == \"classification\":\n",
    "    prop_train, prop_test = reg_to_clf(prop_train), reg_to_clf(prop_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols_train, y_train = [], []\n",
    "for smi, prop in zip(smi_train, prop_train):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol:\n",
    "        mols_train.append(mol)\n",
    "        y_train.append(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols_test, y_test = [], []\n",
    "for smi, prop in zip(smi_test, prop_test):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol:\n",
    "        mols_test.append(mol)\n",
    "        y_test.append(prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conformer generation\n",
    "\n",
    "For each molecule, an ensemble of conformers is generated. Then, molecules for which conformer generation failed are filtered out from both, the training and test set. Generated conformers can be accessed by mol.GetConformers(confID=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qsarmil.conformer import RDKitConformerGenerator\n",
    "from qsarmil.utils.logging import FailedConformer, FailedDescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_gen = RDKitConformerGenerator(num_conf=10, num_cpu=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating conformers: 100%|██████████████████████████████████████████████████████████| 537/537 [00:20<00:00, 26.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate conformers for training molecules\n",
    "confs_train = conf_gen.run(mols_train)\n",
    "\n",
    "# Filter out molecules where conformer generation failed\n",
    "valid = [(c, y) for c, y in zip(confs_train, y_train) if not isinstance(c, FailedConformer)]\n",
    "\n",
    "# Unpack back into separate lists\n",
    "confs_train, y_train = map(list, zip(*valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating conformers: 100%|██████████████████████████████████████████████████████████| 135/135 [00:06<00:00, 21.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate conformers for test molecules\n",
    "confs_test = conf_gen.run(mols_test)\n",
    "\n",
    "# Filter out molecules where conformer generation failed\n",
    "valid = [(c, y) for c, y in zip(confs_test, y_test) if not isinstance(c, FailedConformer)]\n",
    "\n",
    "# Unpack back into separate lists\n",
    "confs_test, y_test = map(list, zip(*valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Descriptor calculation for conformers\n",
    "\n",
    "Once conformers are generated for each molecule, the next step is to compute **3D molecular descriptors** — numerical representations that capture the geometric and physicochemical properties of each conformer.  \n",
    "Since each molecule can have multiple conformers, the resulting data are structured as **bags of descriptor vectors**, where each **bag** corresponds to a molecule and each **instance** within the bag corresponds to a conformer.\n",
    "\n",
    "To streamline this process, a **descriptor wrapper** is used.  \n",
    "This wrapper provides a unified interface for applying multiple descriptor calculators sourced from external packages (e.g., **RDKit** and **MolFeat**). It automatically handles descriptor computation for all conformers in each molecule.\n",
    "\n",
    "In this example, several descriptor types are combined:\n",
    "- **RDKit-based 3D descriptors:** `GEOM`, `AUTOCORR`, `RDF`, `MORSE`, `WHIM`, `GETAWAY`\n",
    "- **MolFeat descriptors:** `Pharmacophore3D`, `USRDescriptors`, `ElectroShapeDescriptors`\n",
    "\n",
    "After computing the raw descriptors, the values are **scaled** using `BagMinMaxScaler` from the `milearn.preprocessing` module.  \n",
    "This scaling step ensures that descriptor values across conformers and molecules are brought to a consistent range, which helps stabilize and improve model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qsarmil.descriptor.rdkit import (RDKitGEOM, \n",
    "                                      RDKitAUTOCORR, \n",
    "                                      RDKitRDF, \n",
    "                                      RDKitMORSE, \n",
    "                                      RDKitWHIM, \n",
    "                                      RDKitGETAWAY)\n",
    "\n",
    "from molfeat.calc import Pharmacophore3D, USRDescriptors, ElectroShapeDescriptors\n",
    "\n",
    "from qsarmil.descriptor.wrapper import DescriptorWrapper\n",
    "\n",
    "from milearn.preprocessing import BagMinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_calc = DescriptorWrapper(Pharmacophore3D(factory=\"pmapper\"), num_cpu=4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating descriptors: 100%|████████████████████████████████████████████████████████| 537/537 [06:25<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = desc_calc.run(confs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating descriptors: 100%|████████████████████████████████████████████████████████| 135/135 [01:48<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "x_test = desc_calc.run(confs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = BagMinMaxScaler()\n",
    "\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Mini-Benchmark: Evaluating Multiple MIL Architectures\n",
    "\n",
    "This section performs a **mini-benchmark** of several **Multiple Instance Learning (MIL)** neural network architectures for both regression and classification tasks.  \n",
    "The goal is to compare different pooling strategies and network types to evaluate how well they model the relationship between bags (molecules) and their instances (conformers).\n",
    "\n",
    "---\n",
    "\n",
    "#### Models Included\n",
    "\n",
    "The benchmark covers multiple families of MIL models:\n",
    "\n",
    "- **Wrapper MIL Networks**  \n",
    "  These models wrap standard MLPs with MIL pooling strategies.  \n",
    "  - `BagWrapperMLPNetwork` – pooling applied to bag embeddings  \n",
    "  - `InstanceWrapperMLPNetwork` – pooling applied to instance embeddings  \n",
    "\n",
    "- **Classic MIL Networks**  \n",
    "  Standard architectures where pooling is directly integrated into the MIL framework.  \n",
    "  - `BagNetwork`  \n",
    "  - `InstanceNetwork`\n",
    "\n",
    "- **Attention-Based MIL Networks**  \n",
    "  Models that learn to assign importance weights to individual conformers.  \n",
    "  - `AdditiveAttentionNetwork`  \n",
    "  - `SelfAttentionNetwork`  \n",
    "  - `HopfieldAttentionNetwork`\n",
    "\n",
    "- **Other MIL Architectures**  \n",
    "  - `DynamicPoolingNetwork` – adaptive pooling mechanism that adjusts to bag composition\n",
    "\n",
    "Each architecture is instantiated separately for **regression** and **classification** versions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Benchmark Procedure\n",
    "\n",
    "1. The list of models is selected depending on the task type (`TASK`):  \n",
    "   - `regressor_list` for regression  \n",
    "   - `classifier_list` for classification  \n",
    "\n",
    "2. Each model is trained on the **scaled training data** (`x_train_scaled`, `y_train`) using `.fit()`.\n",
    "\n",
    "3. Predictions are generated on the test set (`x_test_scaled`):  \n",
    "   - For regression: `model.predict()` returns continuous outputs.  \n",
    "   - For classification: probabilistic outputs are thresholded at 0.5.\n",
    "\n",
    "4. The **performance metric** (here labeled `\"ACC\"`) is computed using the helper function `accuracy_metric()` and stored in a result dataframe `res_df`.\n",
    "\n",
    "This structure allows easy expansion: new architectures or pooling strategies can be added to the benchmark by simply appending them to the corresponding model list.\n",
    "\n",
    "---\n",
    "\n",
    "#### Stepwise Hyperparameter Optimization in *milearn*\n",
    "\n",
    "The *milearn* framework also supports **stepwise hyperparameter optimization** through the `StepwiseHopt` class.  \n",
    "This optimizer explores a predefined hyperparameter grid in a **parameter-by-parameter** manner, identifying the best value for each while keeping previously optimized parameters fixed.\n",
    "\n",
    "Key features:\n",
    "- Parallel evaluation of multiple parameter values via `ThreadPoolExecutor`.  \n",
    "- Automatic handling of `torch` thread allocation for efficient CPU utilization.  \n",
    "- Built-in tracking of training loss, epochs, and elapsed time for each candidate configuration.  \n",
    "- Designed for both **regression** and **classification** MIL models.  \n",
    "\n",
    "Default search space parameters (defined in `DEFAULT_PARAM_GRID`) include:\n",
    "- Network depth and activation functions  \n",
    "- Learning rate, batch size, and weight decay  \n",
    "- MIL-specific parameters (`tau`, `instance_dropout`)  \n",
    "- Random seed for reproducibility  \n",
    "\n",
    "To run hyperparameter optimization before training, simply call:\n",
    "```python\n",
    "model.hopt(x_train_scaled, y_train, param_grid=DEFAULT_PARAM_GRID, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST dataset creation\n",
    "from milearn.data.mnist import load_mnist, create_bags_or, create_bags_and, create_bags_xor, create_bags_reg\n",
    "\n",
    "# Preprocessing\n",
    "from milearn.preprocessing import BagMinMaxScaler\n",
    "\n",
    "# Network hparams\n",
    "from milearn.network.module.hopt import DEFAULT_PARAM_GRID\n",
    "\n",
    "# MIL wrappers\n",
    "from milearn.network.regressor import BagWrapperMLPNetworkRegressor, InstanceWrapperMLPNetworkRegressor\n",
    "from milearn.network.classifier import BagWrapperMLPNetworkClassifier, InstanceWrapperMLPNetworkClassifier\n",
    "\n",
    "# MIL networks\n",
    "from milearn.network.regressor import (InstanceNetworkRegressor,\n",
    "                                       BagNetworkRegressor,\n",
    "                                       AdditiveAttentionNetworkRegressor,\n",
    "                                       SelfAttentionNetworkRegressor,\n",
    "                                       HopfieldAttentionNetworkRegressor,\n",
    "                                       DynamicPoolingNetworkRegressor)\n",
    "\n",
    "from milearn.network.classifier import (InstanceNetworkClassifier,\n",
    "                                        BagNetworkClassifier,\n",
    "                                        AdditiveAttentionNetworkClassifier,\n",
    "                                        SelfAttentionNetworkClassifier,\n",
    "                                        HopfieldAttentionNetworkClassifier,\n",
    "                                        DynamicPoolingNetworkClassifier)\n",
    "\n",
    "# Utils\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_list = [\n",
    "\n",
    "        # wrapper mil networks\n",
    "        (\"MeanBagWrapperMLPNetworkRegressor\", BagWrapperMLPNetworkRegressor(pool=\"mean\")),\n",
    "        (\"MeanInstanceWrapperMLPNetworkRegressor\", InstanceWrapperMLPNetworkRegressor(pool=\"mean\")),\n",
    "    \n",
    "        # classic mil networks\n",
    "        (\"MeanBagNetworkRegressor\", BagNetworkRegressor(pool=\"mean\")),\n",
    "        (\"MeanInstanceNetworkRegressor\", InstanceNetworkRegressor(pool=\"mean\")),\n",
    "\n",
    "        # attention mil networks\n",
    "        (\"AdditiveAttentionNetworkRegressor\", AdditiveAttentionNetworkRegressor()),\n",
    "        (\"SelfAttentionNetworkRegressor\", SelfAttentionNetworkRegressor()),\n",
    "        (\"HopfieldAttentionNetworkRegressor\", HopfieldAttentionNetworkRegressor()),\n",
    "\n",
    "        # other mil networks\n",
    "        (\"DynamicPoolingNetworkRegressor\", DynamicPoolingNetworkRegressor()),\n",
    "    ]\n",
    "\n",
    "classifier_list = [\n",
    "\n",
    "        # wrapper mil networks\n",
    "        (\"MeanBagWrapperMLPNetworkClassifier\", BagWrapperMLPNetworkClassifier(pool=\"mean\")),\n",
    "        (\"MeanInstanceWrapperMLPNetworkClassifier\", InstanceWrapperMLPNetworkClassifier(pool=\"mean\")),\n",
    "    \n",
    "        # classic mil networks\n",
    "        (\"MeanBagNetworkClassifier\", BagNetworkClassifier(pool=\"mean\")),\n",
    "        (\"MeanInstanceNetworkClassifier\", InstanceNetworkClassifier(pool=\"mean\")),\n",
    "\n",
    "        # attention mil networks\n",
    "        (\"AdditiveAttentionNetworkClassifier\", AdditiveAttentionNetworkClassifier()),\n",
    "        (\"SelfAttentionNetworkClassifier\", SelfAttentionNetworkClassifier()),\n",
    "        (\"HopfieldAttentionNetworkClassifier\", HopfieldAttentionNetworkClassifier()),\n",
    "\n",
    "        # other mil networks\n",
    "        (\"DynamicPoolingNetworkClassifier\", DynamicPoolingNetworkClassifier()),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/8] Training MeanBagWrapperMLPNetworkRegressor...\n",
      "Optimizing hyperparameter: hidden_layer_sizes (3 options)\n",
      "[1/28 |  3.6% |  0.2 min] Value: (2048, 1024, 512, 256, 128, 64), Epochs: 41, Loss: 0.7431\n",
      "[2/28 |  7.1% |  0.1 min] Value: (256, 128, 64), Epochs: 46, Loss: 0.7956\n",
      "[3/28 | 10.7% |  0.2 min] Value: (128,), Epochs: 85, Loss: 1.0821\n",
      "Best hidden_layer_sizes = (2048, 1024, 512, 256, 128, 64), val_loss = 0.7431\n",
      "Optimizing hyperparameter: activation (5 options)\n",
      "[4/28 | 14.3% |  2.2 min] Value: relu, Epochs: 49, Loss: 1.1415\n",
      "[5/28 | 17.9% |  2.1 min] Value: leakyrelu, Epochs: 41, Loss: 1.1453\n",
      "[6/28 | 21.4% |  2.1 min] Value: gelu, Epochs: 47, Loss: 0.6880\n",
      "[7/28 | 25.0% |  1.7 min] Value: elu, Epochs: 41, Loss: 0.7689\n",
      "[8/28 | 28.6% |  2.2 min] Value: silu, Epochs: 54, Loss: 0.6819\n",
      "Best activation = silu, val_loss = 0.6819\n",
      "Optimizing hyperparameter: learning_rate (2 options)\n",
      "[9/28 | 32.1% |  6.7 min] Value: 0.0001, Epochs: 131, Loss: 0.6391\n",
      "[10/28 | 35.7% |  3.8 min] Value: 0.001, Epochs: 35, Loss: 0.6846\n",
      "Best learning_rate = 0.0001, val_loss = 0.6391\n",
      "Optimizing hyperparameter: batch_size (3 options)\n",
      "[11/28 | 39.3% | 29.7 min] Value: 32, Epochs: 57, Loss: 0.6435\n",
      "[12/28 | 42.9% | 26.2 min] Value: 512, Epochs: 381, Loss: 0.6436\n",
      "[13/28 | 46.4% | 28.2 min] Value: 1024, Epochs: 415, Loss: 0.6355\n",
      "Best batch_size = 1024, val_loss = 0.6355\n",
      "Optimizing hyperparameter: weight_decay (5 options)\n",
      "[14/28 | 50.0% | 25.8 min] Value: 0.0, Epochs: 387, Loss: 0.6427\n",
      "[15/28 | 53.6% | 26.4 min] Value: 1e-05, Epochs: 387, Loss: 0.6544\n",
      "[16/28 | 57.1% | 26.5 min] Value: 0.0001, Epochs: 409, Loss: 0.6531\n",
      "[17/28 | 60.7% | 26.3 min] Value: 0.001, Epochs: 383, Loss: 0.6373\n",
      "[18/28 | 64.3% | 26.2 min] Value: 0.01, Epochs: 376, Loss: 0.6398\n",
      "Best weight_decay = 0.001, val_loss = 0.6373\n",
      "Optimizing hyperparameter: instance_dropout (5 options)\n",
      "[19/28 | 67.9% | 25.4 min] Value: 0.0, Epochs: 393, Loss: 0.6406\n",
      "[20/28 | 71.4% | 23.9 min] Value: 0.2, Epochs: 352, Loss: 0.6532\n",
      "[21/28 | 75.0% | 25.2 min] Value: 0.4, Epochs: 384, Loss: 0.6462\n",
      "[22/28 | 78.6% | 24.8 min] Value: 0.6, Epochs: 373, Loss: 0.6397\n",
      "[23/28 | 82.1% | 25.4 min] Value: 0.8, Epochs: 413, Loss: 0.6377\n",
      "Best instance_dropout = 0.8, val_loss = 0.6377\n",
      "Optimizing hyperparameter: random_seed (5 options)\n",
      "[24/28 | 85.7% | 22.1 min] Value: 1, Epochs: 361, Loss: 0.6417\n",
      "[25/28 | 89.3% | 22.4 min] Value: 2, Epochs: 367, Loss: 0.6429\n",
      "[26/28 | 92.9% | 22.6 min] Value: 3, Epochs: 376, Loss: 0.6413\n",
      "[27/28 | 96.4% | 22.6 min] Value: 4, Epochs: 395, Loss: 0.6340\n",
      "[28/28 | 100.0% | 22.4 min] Value: 5, Epochs: 370, Loss: 0.6370\n",
      "Best random_seed = 4, val_loss = 0.6340\n",
      "Stepwise optimization completed in 113.3 min\n",
      "\n",
      "→ MeanBagWrapperMLPNetworkRegressor training complete. Evaluating...\n",
      "✓ MeanBagWrapperMLPNetworkRegressor done — ACC: 0.5528\n",
      "\n",
      "[2/8] Training MeanInstanceWrapperMLPNetworkRegressor...\n",
      "Optimizing hyperparameter: hidden_layer_sizes (3 options)\n",
      "[1/28 |  3.6% | 16.2 min] Value: (2048, 1024, 512, 256, 128, 64), Epochs: 33, Loss: 0.1276\n",
      "[2/28 |  7.1% |  7.2 min] Value: (256, 128, 64), Epochs: 36, Loss: 0.1384\n",
      "[3/28 | 10.7% |  6.4 min] Value: (128,), Epochs: 53, Loss: 0.2148\n",
      "Best hidden_layer_sizes = (2048, 1024, 512, 256, 128, 64), val_loss = 0.1276\n",
      "Optimizing hyperparameter: activation (5 options)\n",
      "[4/28 | 14.3% | 12.3 min] Value: relu, Epochs: 28, Loss: 0.1711\n",
      "[5/28 | 17.9% | 18.3 min] Value: leakyrelu, Epochs: 43, Loss: 0.1523\n",
      "[6/28 | 21.4% | 22.6 min] Value: gelu, Epochs: 51, Loss: 0.1287\n",
      "[7/28 | 25.0% | 60.8 min] Value: elu, Epochs: 211, Loss: 0.1080\n",
      "[8/28 | 28.6% | 34.8 min] Value: silu, Epochs: 92, Loss: 0.1240\n",
      "Best activation = elu, val_loss = 0.1080\n",
      "Optimizing hyperparameter: learning_rate (2 options)\n",
      "[9/28 | 32.1% | 58.4 min] Value: 0.0001, Epochs: 234, Loss: 0.1878\n",
      "[10/28 | 35.7% | 50.2 min] Value: 0.001, Epochs: 114, Loss: 0.1264\n",
      "Best learning_rate = 0.001, val_loss = 0.1264\n",
      "Optimizing hyperparameter: batch_size (3 options)\n",
      "[11/28 | 39.3% |  9.0 min] Value: 32, Epochs: 51, Loss: 0.1132\n",
      "[12/28 | 42.9% |  2.9 min] Value: 512, Epochs: 74, Loss: 0.1788\n",
      "[13/28 | 46.4% |  4.1 min] Value: 1024, Epochs: 159, Loss: 0.1750\n",
      "Best batch_size = 32, val_loss = 0.1132\n",
      "Optimizing hyperparameter: weight_decay (5 options)\n",
      "[14/28 | 50.0% | 31.8 min] Value: 0.0, Epochs: 70, Loss: 0.1971\n",
      "[15/28 | 53.6% | 17.4 min] Value: 1e-05, Epochs: 32, Loss: 0.1504\n",
      "[16/28 | 57.1% | 41.2 min] Value: 0.0001, Epochs: 115, Loss: 0.0933\n",
      "[17/28 | 60.7% | 36.9 min] Value: 0.001, Epochs: 86, Loss: 0.1064\n",
      "[18/28 | 64.3% | 24.5 min] Value: 0.01, Epochs: 49, Loss: 0.3679\n",
      "Best weight_decay = 0.0001, val_loss = 0.0933\n",
      "Optimizing hyperparameter: instance_dropout (5 options)\n",
      "[19/28 | 67.9% | 19.4 min] Value: 0.0, Epochs: 44, Loss: 0.1437\n",
      "[20/28 | 71.4% | 18.5 min] Value: 0.2, Epochs: 42, Loss: 0.1320\n",
      "[21/28 | 75.0% | 17.7 min] Value: 0.4, Epochs: 37, Loss: 0.1674\n",
      "[22/28 | 78.6% | 20.4 min] Value: 0.6, Epochs: 51, Loss: 0.1515\n",
      "[23/28 | 82.1% | 21.6 min] Value: 0.8, Epochs: 60, Loss: 0.1618\n",
      "Best instance_dropout = 0.2, val_loss = 0.1320\n",
      "Optimizing hyperparameter: random_seed (5 options)\n",
      "[24/28 | 85.7% | 22.2 min] Value: 1, Epochs: 57, Loss: 0.1764\n",
      "[25/28 | 89.3% | 23.7 min] Value: 2, Epochs: 60, Loss: 0.1499\n",
      "[26/28 | 92.9% | 15.1 min] Value: 3, Epochs: 34, Loss: 0.1272\n",
      "[27/28 | 96.4% | 23.5 min] Value: 4, Epochs: 63, Loss: 0.1036\n",
      "[28/28 | 100.0% | 23.6 min] Value: 5, Epochs: 63, Loss: 0.1764\n",
      "Best random_seed = 4, val_loss = 0.1036\n",
      "Stepwise optimization completed in 231.1 min\n",
      "\n",
      "→ MeanInstanceWrapperMLPNetworkRegressor training complete. Evaluating...\n",
      "✓ MeanInstanceWrapperMLPNetworkRegressor done — ACC: 0.5880\n",
      "\n",
      "[3/8] Training MeanBagNetworkRegressor...\n",
      "Optimizing hyperparameter: hidden_layer_sizes (3 options)\n",
      "[1/28 |  3.6% |  0.5 min] Value: (2048, 1024, 512, 256, 128, 64), Epochs: 39, Loss: 0.7489\n",
      "[2/28 |  7.1% |  0.1 min] Value: (256, 128, 64), Epochs: 26, Loss: 1.0499\n",
      "[3/28 | 10.7% |  0.2 min] Value: (128,), Epochs: 57, Loss: 1.1182\n",
      "Best hidden_layer_sizes = (2048, 1024, 512, 256, 128, 64), val_loss = 0.7489\n",
      "Optimizing hyperparameter: activation (5 options)\n",
      "[4/28 | 14.3% |  1.1 min] Value: relu, Epochs: 35, Loss: 0.9016\n",
      "[5/28 | 17.9% |  1.1 min] Value: leakyrelu, Epochs: 33, Loss: 0.8705\n",
      "[6/28 | 21.4% |  1.1 min] Value: gelu, Epochs: 31, Loss: 0.7225\n",
      "[7/28 | 25.0% |  1.2 min] Value: elu, Epochs: 42, Loss: 0.8905\n",
      "[8/28 | 28.6% |  1.2 min] Value: silu, Epochs: 38, Loss: 0.7662\n",
      "Best activation = gelu, val_loss = 0.7225\n",
      "Optimizing hyperparameter: learning_rate (2 options)\n",
      "[9/28 | 32.1% |  0.6 min] Value: 0.0001, Epochs: 38, Loss: 0.7706\n",
      "[10/28 | 35.7% |  0.8 min] Value: 0.001, Epochs: 51, Loss: 0.7505\n",
      "Best learning_rate = 0.001, val_loss = 0.7505\n",
      "Optimizing hyperparameter: batch_size (3 options)\n",
      "[11/28 | 39.3% |  1.0 min] Value: 32, Epochs: 22, Loss: 0.6825\n",
      "[12/28 | 42.9% |  0.9 min] Value: 512, Epochs: 55, Loss: 0.7425\n",
      "[13/28 | 46.4% |  1.0 min] Value: 1024, Epochs: 75, Loss: 0.7400\n",
      "Best batch_size = 32, val_loss = 0.6825\n",
      "Optimizing hyperparameter: weight_decay (5 options)\n",
      "[14/28 | 50.0% |  0.9 min] Value: 0.0, Epochs: 18, Loss: 0.7569\n",
      "[15/28 | 53.6% |  1.2 min] Value: 1e-05, Epochs: 25, Loss: 0.7323\n",
      "[16/28 | 57.1% |  1.1 min] Value: 0.0001, Epochs: 23, Loss: 0.7751\n",
      "[17/28 | 60.7% |  1.2 min] Value: 0.001, Epochs: 26, Loss: 0.7519\n",
      "[18/28 | 64.3% |  1.0 min] Value: 0.01, Epochs: 19, Loss: 0.8157\n",
      "Best weight_decay = 1e-05, val_loss = 0.7323\n",
      "Optimizing hyperparameter: instance_dropout (5 options)\n",
      "[19/28 | 67.9% |  1.3 min] Value: 0.0, Epochs: 21, Loss: 0.8044\n",
      "[20/28 | 71.4% |  1.6 min] Value: 0.2, Epochs: 32, Loss: 0.7359\n",
      "[21/28 | 75.0% |  1.6 min] Value: 0.4, Epochs: 30, Loss: 0.7363\n",
      "[22/28 | 78.6% |  1.3 min] Value: 0.6, Epochs: 23, Loss: 0.8440\n",
      "[23/28 | 82.1% |  1.9 min] Value: 0.8, Epochs: 42, Loss: 0.7181\n",
      "Best instance_dropout = 0.8, val_loss = 0.7181\n",
      "Optimizing hyperparameter: random_seed (5 options)\n",
      "[24/28 | 85.7% |  2.8 min] Value: 1, Epochs: 66, Loss: 0.7118\n",
      "[25/28 | 89.3% |  1.8 min] Value: 2, Epochs: 30, Loss: 0.6793\n",
      "[26/28 | 92.9% |  2.2 min] Value: 3, Epochs: 40, Loss: 0.6738\n",
      "[27/28 | 96.4% |  2.2 min] Value: 4, Epochs: 39, Loss: 0.7165\n",
      "[28/28 | 100.0% |  2.2 min] Value: 5, Epochs: 37, Loss: 0.7385\n",
      "Best random_seed = 3, val_loss = 0.6738\n",
      "Stepwise optimization completed in 9.4 min\n",
      "\n",
      "→ MeanBagNetworkRegressor training complete. Evaluating...\n",
      "✓ MeanBagNetworkRegressor done — ACC: 0.5777\n",
      "\n",
      "[4/8] Training MeanInstanceNetworkRegressor...\n",
      "Optimizing hyperparameter: hidden_layer_sizes (3 options)\n",
      "[1/28 |  3.6% |  0.5 min] Value: (2048, 1024, 512, 256, 128, 64), Epochs: 39, Loss: 0.7798\n",
      "[2/28 |  7.1% |  0.2 min] Value: (256, 128, 64), Epochs: 37, Loss: 1.0252\n",
      "[3/28 | 10.7% |  0.2 min] Value: (128,), Epochs: 74, Loss: 1.1158\n",
      "Best hidden_layer_sizes = (2048, 1024, 512, 256, 128, 64), val_loss = 0.7798\n",
      "Optimizing hyperparameter: activation (5 options)\n",
      "[4/28 | 14.3% |  1.0 min] Value: relu, Epochs: 40, Loss: 0.8626\n",
      "[5/28 | 17.9% |  1.0 min] Value: leakyrelu, Epochs: 36, Loss: 0.8657\n",
      "[6/28 | 21.4% |  0.8 min] Value: gelu, Epochs: 26, Loss: 0.7528\n",
      "[7/28 | 25.0% |  1.2 min] Value: elu, Epochs: 53, Loss: 0.7527\n",
      "[8/28 | 28.6% |  0.9 min] Value: silu, Epochs: 30, Loss: 0.6971\n",
      "Best activation = silu, val_loss = 0.6971\n",
      "Optimizing hyperparameter: learning_rate (2 options)\n",
      "[9/28 | 32.1% |  0.6 min] Value: 0.0001, Epochs: 47, Loss: 0.6736\n",
      "[10/28 | 35.7% |  0.5 min] Value: 0.001, Epochs: 33, Loss: 0.7530\n",
      "Best learning_rate = 0.0001, val_loss = 0.6736\n",
      "Optimizing hyperparameter: batch_size (3 options)\n",
      "[11/28 | 39.3% |  1.2 min] Value: 32, Epochs: 27, Loss: 0.7376\n",
      "[12/28 | 42.9% |  2.0 min] Value: 512, Epochs: 150, Loss: 0.6739\n",
      "[13/28 | 46.4% |  2.1 min] Value: 1024, Epochs: 150, Loss: 0.6739\n",
      "Best batch_size = 512, val_loss = 0.6739\n",
      "Optimizing hyperparameter: weight_decay (5 options)\n",
      "[14/28 | 50.0% |  3.7 min] Value: 0.0, Epochs: 150, Loss: 0.6739\n",
      "[15/28 | 53.6% |  3.6 min] Value: 1e-05, Epochs: 150, Loss: 0.6739\n",
      "[16/28 | 57.1% |  3.2 min] Value: 0.0001, Epochs: 121, Loss: 0.7810\n",
      "[17/28 | 60.7% |  3.7 min] Value: 0.001, Epochs: 150, Loss: 0.6739\n",
      "[18/28 | 64.3% |  3.7 min] Value: 0.01, Epochs: 150, Loss: 0.6739\n",
      "Best weight_decay = 0.0, val_loss = 0.6739\n",
      "Optimizing hyperparameter: instance_dropout (5 options)\n",
      "[19/28 | 67.9% |  3.3 min] Value: 0.0, Epochs: 147, Loss: 0.7021\n",
      "[20/28 | 71.4% |  3.4 min] Value: 0.2, Epochs: 153, Loss: 0.6740\n",
      "[21/28 | 75.0% |  2.9 min] Value: 0.4, Epochs: 125, Loss: 0.7356\n",
      "[22/28 | 78.6% |  3.5 min] Value: 0.6, Epochs: 159, Loss: 0.6801\n",
      "[23/28 | 82.1% |  3.5 min] Value: 0.8, Epochs: 160, Loss: 0.6675\n",
      "Best instance_dropout = 0.8, val_loss = 0.6675\n",
      "Optimizing hyperparameter: random_seed (5 options)\n",
      "[24/28 | 85.7% |  3.4 min] Value: 1, Epochs: 159, Loss: 0.7292\n",
      "[25/28 | 89.3% |  3.1 min] Value: 2, Epochs: 135, Loss: 0.7361\n",
      "[26/28 | 92.9% |  3.3 min] Value: 3, Epochs: 150, Loss: 0.6908\n",
      "[27/28 | 96.4% |  3.1 min] Value: 4, Epochs: 138, Loss: 0.7160\n",
      "[28/28 | 100.0% |  2.8 min] Value: 5, Epochs: 120, Loss: 0.7748\n",
      "Best random_seed = 3, val_loss = 0.6908\n",
      "Stepwise optimization completed in 14.9 min\n",
      "\n",
      "→ MeanInstanceNetworkRegressor training complete. Evaluating...\n",
      "✓ MeanInstanceNetworkRegressor done — ACC: 0.5108\n",
      "\n",
      "[5/8] Training AdditiveAttentionNetworkRegressor...\n",
      "Optimizing hyperparameter: hidden_layer_sizes (3 options)\n",
      "[1/31 |  3.2% |  0.3 min] Value: (2048, 1024, 512, 256, 128, 64), Epochs: 27, Loss: 1.1952\n",
      "[2/31 |  6.5% |  0.1 min] Value: (256, 128, 64), Epochs: 30, Loss: 1.0207\n",
      "[3/31 |  9.7% |  0.2 min] Value: (128,), Epochs: 66, Loss: 1.1807\n",
      "Best hidden_layer_sizes = (256, 128, 64), val_loss = 1.0207\n",
      "Optimizing hyperparameter: activation (5 options)\n",
      "[4/31 | 12.9% |  0.2 min] Value: relu, Epochs: 34, Loss: 1.0473\n",
      "[5/31 | 16.1% |  0.2 min] Value: leakyrelu, Epochs: 37, Loss: 1.1201\n",
      "[6/31 | 19.4% |  0.2 min] Value: gelu, Epochs: 31, Loss: 1.0321\n",
      "[7/31 | 22.6% |  0.2 min] Value: elu, Epochs: 44, Loss: 0.8067\n",
      "[8/31 | 25.8% |  0.2 min] Value: silu, Epochs: 30, Loss: 0.8092\n",
      "Best activation = elu, val_loss = 0.8067\n",
      "Optimizing hyperparameter: learning_rate (2 options)\n",
      "[9/31 | 29.0% |  0.3 min] Value: 0.0001, Epochs: 97, Loss: 1.0393\n",
      "[10/31 | 32.3% |  0.2 min] Value: 0.001, Epochs: 38, Loss: 0.7807\n",
      "Best learning_rate = 0.001, val_loss = 0.7807\n",
      "Optimizing hyperparameter: batch_size (3 options)\n",
      "[11/31 | 35.5% |  0.2 min] Value: 32, Epochs: 21, Loss: 0.8036\n",
      "[12/31 | 38.7% |  0.3 min] Value: 512, Epochs: 114, Loss: 0.7532\n",
      "[13/31 | 41.9% |  0.3 min] Value: 1024, Epochs: 114, Loss: 0.7532\n",
      "Best batch_size = 1024, val_loss = 0.7532\n",
      "Optimizing hyperparameter: weight_decay (5 options)\n",
      "[14/31 | 45.2% |  0.4 min] Value: 0.0, Epochs: 103, Loss: 0.6953\n",
      "[15/31 | 48.4% |  0.4 min] Value: 1e-05, Epochs: 114, Loss: 0.7532\n",
      "[16/31 | 51.6% |  0.4 min] Value: 0.0001, Epochs: 114, Loss: 0.7532\n",
      "[17/31 | 54.8% |  0.4 min] Value: 0.001, Epochs: 114, Loss: 0.7532\n",
      "[18/31 | 58.1% |  0.4 min] Value: 0.01, Epochs: 114, Loss: 0.7529\n",
      "Best weight_decay = 0.0, val_loss = 0.6953\n",
      "Optimizing hyperparameter: tau (3 options)\n",
      "[19/31 | 61.3% |  0.3 min] Value: 0.01, Epochs: 118, Loss: 0.8882\n",
      "[20/31 | 64.5% |  0.3 min] Value: 0.5, Epochs: 115, Loss: 0.7676\n",
      "[21/31 | 67.7% |  0.3 min] Value: 1.0, Epochs: 114, Loss: 0.7532\n",
      "Best tau = 1.0, val_loss = 0.7532\n",
      "Optimizing hyperparameter: instance_dropout (5 options)\n",
      "[22/31 | 71.0% |  0.4 min] Value: 0.0, Epochs: 114, Loss: 0.7532\n",
      "[23/31 | 74.2% |  0.4 min] Value: 0.2, Epochs: 116, Loss: 0.7516\n",
      "[24/31 | 77.4% |  0.4 min] Value: 0.4, Epochs: 118, Loss: 0.7425\n",
      "[25/31 | 80.6% |  0.4 min] Value: 0.6, Epochs: 116, Loss: 0.7313\n",
      "[26/31 | 83.9% |  0.4 min] Value: 0.8, Epochs: 112, Loss: 0.7335\n",
      "Best instance_dropout = 0.6, val_loss = 0.7313\n",
      "Optimizing hyperparameter: random_seed (5 options)\n",
      "[27/31 | 87.1% |  0.3 min] Value: 1, Epochs: 96, Loss: 0.6817\n",
      "[28/31 | 90.3% |  0.4 min] Value: 2, Epochs: 104, Loss: 0.6898\n",
      "[29/31 | 93.5% |  0.4 min] Value: 3, Epochs: 104, Loss: 0.6541\n",
      "[30/31 | 96.8% |  0.4 min] Value: 4, Epochs: 121, Loss: 0.6936\n",
      "[31/31 | 100.0% |  0.3 min] Value: 5, Epochs: 83, Loss: 0.7263\n",
      "Best random_seed = 3, val_loss = 0.6541\n",
      "Stepwise optimization completed in 2.7 min\n",
      "\n",
      "→ AdditiveAttentionNetworkRegressor training complete. Evaluating...\n",
      "✓ AdditiveAttentionNetworkRegressor done — ACC: 0.5223\n",
      "\n",
      "[6/8] Training SelfAttentionNetworkRegressor...\n",
      "Optimizing hyperparameter: hidden_layer_sizes (3 options)\n",
      "[1/31 |  3.2% |  0.6 min] Value: (2048, 1024, 512, 256, 128, 64), Epochs: 46, Loss: 0.7559\n",
      "[2/31 |  6.5% |  0.1 min] Value: (256, 128, 64), Epochs: 31, Loss: 0.9586\n",
      "[3/31 |  9.7% |  0.1 min] Value: (128,), Epochs: 41, Loss: 1.0288\n",
      "Best hidden_layer_sizes = (2048, 1024, 512, 256, 128, 64), val_loss = 0.7559\n",
      "Optimizing hyperparameter: activation (5 options)\n",
      "[4/31 | 12.9% |  0.9 min] Value: relu, Epochs: 26, Loss: 0.9575\n",
      "[5/31 | 16.1% |  1.4 min] Value: leakyrelu, Epochs: 42, Loss: 0.9394\n",
      "[6/31 | 19.4% |  1.2 min] Value: gelu, Epochs: 37, Loss: 0.7874\n",
      "[7/31 | 22.6% |  1.7 min] Value: elu, Epochs: 54, Loss: 0.8079\n",
      "[8/31 | 25.8% |  1.0 min] Value: silu, Epochs: 28, Loss: 0.7331\n",
      "Best activation = silu, val_loss = 0.7331\n",
      "Optimizing hyperparameter: learning_rate (2 options)\n",
      "[9/31 | 29.0% |  0.6 min] Value: 0.0001, Epochs: 38, Loss: 0.7516\n",
      "[10/31 | 32.3% |  0.5 min] Value: 0.001, Epochs: 29, Loss: 0.7525\n",
      "Best learning_rate = 0.0001, val_loss = 0.7516\n",
      "Optimizing hyperparameter: batch_size (3 options)\n",
      "[11/31 | 35.5% |  1.3 min] Value: 32, Epochs: 22, Loss: 0.7823\n",
      "[12/31 | 38.7% |  1.7 min] Value: 512, Epochs: 112, Loss: 0.7458\n",
      "[13/31 | 41.9% |  1.7 min] Value: 1024, Epochs: 112, Loss: 0.7458\n",
      "Best batch_size = 512, val_loss = 0.7458\n",
      "Optimizing hyperparameter: weight_decay (5 options)\n",
      "[14/31 | 45.2% |  2.3 min] Value: 0.0, Epochs: 91, Loss: 0.8250\n",
      "[15/31 | 48.4% |  2.8 min] Value: 1e-05, Epochs: 112, Loss: 0.7458\n",
      "[16/31 | 51.6% |  2.8 min] Value: 0.0001, Epochs: 120, Loss: 0.7227\n",
      "[17/31 | 54.8% |  2.8 min] Value: 0.001, Epochs: 112, Loss: 0.7458\n",
      "[18/31 | 58.1% |  2.8 min] Value: 0.01, Epochs: 112, Loss: 0.7457\n",
      "Best weight_decay = 0.0001, val_loss = 0.7227\n",
      "Optimizing hyperparameter: tau (3 options)\n",
      "[19/31 | 61.3% |  3.9 min] Value: 0.01, Epochs: 124, Loss: 0.7763\n",
      "[20/31 | 64.5% |  3.5 min] Value: 0.5, Epochs: 225, Loss: 0.7615\n",
      "[21/31 | 67.7% |  2.2 min] Value: 1.0, Epochs: 112, Loss: 0.7458\n",
      "Best tau = 1.0, val_loss = 0.7458\n",
      "Optimizing hyperparameter: instance_dropout (5 options)\n",
      "[22/31 | 71.0% |  2.6 min] Value: 0.0, Epochs: 112, Loss: 0.7458\n",
      "[23/31 | 74.2% |  3.4 min] Value: 0.2, Epochs: 166, Loss: 0.7053\n",
      "[24/31 | 77.4% |  2.5 min] Value: 0.4, Epochs: 109, Loss: 0.7355\n",
      "[25/31 | 80.6% |  2.5 min] Value: 0.6, Epochs: 100, Loss: 0.7759\n",
      "[26/31 | 83.9% |  2.5 min] Value: 0.8, Epochs: 102, Loss: 0.7882\n",
      "Best instance_dropout = 0.2, val_loss = 0.7053\n",
      "Optimizing hyperparameter: random_seed (5 options)\n",
      "[27/31 | 87.1% |  2.6 min] Value: 1, Epochs: 105, Loss: 0.7766\n",
      "[28/31 | 90.3% |  2.5 min] Value: 2, Epochs: 97, Loss: 0.8452\n",
      "[29/31 | 93.5% |  2.7 min] Value: 3, Epochs: 112, Loss: 0.7856\n",
      "[30/31 | 96.8% |  2.7 min] Value: 4, Epochs: 109, Loss: 0.7378\n",
      "[31/31 | 100.0% |  2.1 min] Value: 5, Epochs: 82, Loss: 0.8815\n",
      "Best random_seed = 4, val_loss = 0.7378\n",
      "Stepwise optimization completed in 17.4 min\n",
      "\n",
      "→ SelfAttentionNetworkRegressor training complete. Evaluating...\n",
      "✓ SelfAttentionNetworkRegressor done — ACC: 0.4310\n",
      "\n",
      "[7/8] Training HopfieldAttentionNetworkRegressor...\n",
      "Optimizing hyperparameter: hidden_layer_sizes (3 options)\n",
      "[1/31 |  3.2% |  0.5 min] Value: (2048, 1024, 512, 256, 128, 64), Epochs: 35, Loss: 0.7424\n",
      "[2/31 |  6.5% |  0.2 min] Value: (256, 128, 64), Epochs: 39, Loss: 1.0353\n",
      "[3/31 |  9.7% |  0.2 min] Value: (128,), Epochs: 48, Loss: 1.3264\n",
      "Best hidden_layer_sizes = (2048, 1024, 512, 256, 128, 64), val_loss = 0.7424\n",
      "Optimizing hyperparameter: activation (5 options)\n",
      "[4/31 | 12.9% |  0.8 min] Value: relu, Epochs: 27, Loss: 0.9053\n",
      "[5/31 | 16.1% |  0.8 min] Value: leakyrelu, Epochs: 25, Loss: 0.9310\n",
      "[6/31 | 19.4% |  0.9 min] Value: gelu, Epochs: 30, Loss: 0.7776\n",
      "[7/31 | 22.6% |  1.3 min] Value: elu, Epochs: 55, Loss: 0.9133\n",
      "[8/31 | 25.8% |  0.9 min] Value: silu, Epochs: 31, Loss: 0.7391\n",
      "Best activation = silu, val_loss = 0.7391\n",
      "Optimizing hyperparameter: learning_rate (2 options)\n",
      "[9/31 | 29.0% |  0.8 min] Value: 0.0001, Epochs: 50, Loss: 0.7394\n",
      "[10/31 | 32.3% |  0.6 min] Value: 0.001, Epochs: 33, Loss: 0.7706\n",
      "Best learning_rate = 0.0001, val_loss = 0.7394\n",
      "Optimizing hyperparameter: batch_size (3 options)\n",
      "[11/31 | 35.5% |  1.0 min] Value: 32, Epochs: 23, Loss: 0.7720\n",
      "[12/31 | 38.7% |  1.6 min] Value: 512, Epochs: 122, Loss: 0.8027\n",
      "[13/31 | 41.9% |  1.5 min] Value: 1024, Epochs: 116, Loss: 0.7555\n",
      "Best batch_size = 1024, val_loss = 0.7555\n",
      "Optimizing hyperparameter: weight_decay (5 options)\n",
      "[14/31 | 45.2% |  2.9 min] Value: 0.0, Epochs: 107, Loss: 0.8437\n",
      "[15/31 | 48.4% |  3.1 min] Value: 1e-05, Epochs: 121, Loss: 0.8484\n",
      "[16/31 | 51.6% |  4.2 min] Value: 0.0001, Epochs: 193, Loss: 0.7248\n",
      "[17/31 | 54.8% |  3.4 min] Value: 0.001, Epochs: 138, Loss: 0.7105\n",
      "[18/31 | 58.1% |  3.7 min] Value: 0.01, Epochs: 157, Loss: 0.7146\n",
      "Best weight_decay = 0.001, val_loss = 0.7105\n",
      "Optimizing hyperparameter: tau (3 options)\n",
      "[19/31 | 61.3% |  2.5 min] Value: 0.01, Epochs: 150, Loss: 0.6742\n",
      "[20/31 | 64.5% |  2.4 min] Value: 0.5, Epochs: 146, Loss: 0.6922\n",
      "[21/31 | 67.7% |  2.2 min] Value: 1.0, Epochs: 126, Loss: 0.7775\n",
      "Best tau = 0.01, val_loss = 0.6742\n",
      "Optimizing hyperparameter: instance_dropout (5 options)\n",
      "[22/31 | 71.0% |  4.1 min] Value: 0.0, Epochs: 150, Loss: 0.6734\n",
      "[23/31 | 74.2% |  4.0 min] Value: 0.2, Epochs: 140, Loss: 0.6767\n",
      "[24/31 | 77.4% |  4.0 min] Value: 0.4, Epochs: 144, Loss: 0.6718\n",
      "[25/31 | 80.6% |  4.2 min] Value: 0.6, Epochs: 152, Loss: 0.6663\n",
      "[26/31 | 83.9% |  4.3 min] Value: 0.8, Epochs: 161, Loss: 0.6780\n",
      "Best instance_dropout = 0.6, val_loss = 0.6663\n",
      "Optimizing hyperparameter: random_seed (5 options)\n",
      "[27/31 | 87.1% |  3.6 min] Value: 1, Epochs: 151, Loss: 0.7112\n",
      "[28/31 | 90.3% |  3.3 min] Value: 2, Epochs: 129, Loss: 0.7455\n",
      "[29/31 | 93.5% |  3.4 min] Value: 3, Epochs: 139, Loss: 0.6932\n",
      "[30/31 | 96.8% |  3.2 min] Value: 4, Epochs: 124, Loss: 0.7225\n",
      "[31/31 | 100.0% |  3.7 min] Value: 5, Epochs: 156, Loss: 0.7844\n",
      "Best random_seed = 3, val_loss = 0.6932\n",
      "Stepwise optimization completed in 18.8 min\n",
      "\n",
      "→ HopfieldAttentionNetworkRegressor training complete. Evaluating...\n",
      "✓ HopfieldAttentionNetworkRegressor done — ACC: 0.5129\n",
      "\n",
      "[8/8] Training DynamicPoolingNetworkRegressor...\n",
      "Optimizing hyperparameter: hidden_layer_sizes (3 options)\n",
      "[1/28 |  3.6% |  0.4 min] Value: (2048, 1024, 512, 256, 128, 64), Epochs: 33, Loss: 0.0225\n",
      "[2/28 |  7.1% |  0.2 min] Value: (256, 128, 64), Epochs: 34, Loss: 0.0206\n",
      "[3/28 | 10.7% |  0.2 min] Value: (128,), Epochs: 45, Loss: 0.0677\n",
      "Best hidden_layer_sizes = (256, 128, 64), val_loss = 0.0206\n",
      "Optimizing hyperparameter: activation (5 options)\n",
      "[4/28 | 14.3% |  0.1 min] Value: relu, Epochs: 21, Loss: 0.0259\n",
      "[5/28 | 17.9% |  0.1 min] Value: leakyrelu, Epochs: 21, Loss: 0.0265\n",
      "[6/28 | 21.4% |  0.1 min] Value: gelu, Epochs: 25, Loss: 0.0238\n",
      "[7/28 | 25.0% |  0.2 min] Value: elu, Epochs: 46, Loss: 0.0702\n",
      "[8/28 | 28.6% |  0.2 min] Value: silu, Epochs: 30, Loss: 0.0216\n",
      "Best activation = silu, val_loss = 0.0216\n",
      "Optimizing hyperparameter: learning_rate (2 options)\n",
      "[9/28 | 32.1% |  0.1 min] Value: 0.0001, Epochs: 27, Loss: 0.0279\n",
      "[10/28 | 35.7% |  0.1 min] Value: 0.001, Epochs: 19, Loss: 0.0215\n",
      "Best learning_rate = 0.001, val_loss = 0.0215\n",
      "Optimizing hyperparameter: batch_size (3 options)\n",
      "[11/28 | 39.3% |  0.3 min] Value: 32, Epochs: 51, Loss: 0.0202\n",
      "[12/28 | 42.9% |  0.1 min] Value: 512, Epochs: 40, Loss: 0.0205\n",
      "[13/28 | 46.4% |  0.1 min] Value: 1024, Epochs: 30, Loss: 0.0221\n",
      "Best batch_size = 32, val_loss = 0.0202\n",
      "Optimizing hyperparameter: weight_decay (5 options)\n",
      "[14/28 | 50.0% |  0.3 min] Value: 0.0, Epochs: 23, Loss: 0.0216\n",
      "[15/28 | 53.6% |  0.4 min] Value: 1e-05, Epochs: 27, Loss: 0.0204\n",
      "[16/28 | 57.1% |  0.4 min] Value: 0.0001, Epochs: 26, Loss: 0.0204\n",
      "[17/28 | 60.7% |  0.4 min] Value: 0.001, Epochs: 41, Loss: 0.0208\n",
      "[18/28 | 64.3% |  0.4 min] Value: 0.01, Epochs: 33, Loss: 0.0205\n",
      "Best weight_decay = 1e-05, val_loss = 0.0204\n",
      "Optimizing hyperparameter: instance_dropout (5 options)\n",
      "[19/28 | 67.9% |  0.4 min] Value: 0.0, Epochs: 29, Loss: 0.0222\n",
      "[20/28 | 71.4% |  0.2 min] Value: 0.2, Epochs: 14, Loss: 0.0252\n",
      "[21/28 | 75.0% |  0.3 min] Value: 0.4, Epochs: 25, Loss: 0.0417\n",
      "[22/28 | 78.6% |  0.3 min] Value: 0.6, Epochs: 22, Loss: 0.0828\n",
      "[23/28 | 82.1% |  0.4 min] Value: 0.8, Epochs: 27, Loss: 0.1250\n",
      "Best instance_dropout = 0.0, val_loss = 0.0222\n",
      "Optimizing hyperparameter: random_seed (5 options)\n",
      "[24/28 | 85.7% |  0.4 min] Value: 1, Epochs: 25, Loss: 0.0199\n",
      "[25/28 | 89.3% |  0.4 min] Value: 2, Epochs: 23, Loss: 0.0206\n",
      "[26/28 | 92.9% |  0.5 min] Value: 3, Epochs: 30, Loss: 0.0198\n",
      "[27/28 | 96.4% |  0.5 min] Value: 4, Epochs: 38, Loss: 0.0224\n",
      "[28/28 | 100.0% |  0.4 min] Value: 5, Epochs: 24, Loss: 0.0222\n",
      "Best random_seed = 3, val_loss = 0.0198\n",
      "Stepwise optimization completed in 2.3 min\n",
      "\n",
      "→ DynamicPoolingNetworkRegressor training complete. Evaluating...\n",
      "✓ DynamicPoolingNetworkRegressor done — ACC: 0.5749\n"
     ]
    }
   ],
   "source": [
    "if TASK == \"regression\":\n",
    "    method_list = regressor_list\n",
    "elif TASK == \"classification\":\n",
    "    method_list = classifier_list\n",
    "\n",
    "res_df = pd.DataFrame()\n",
    "\n",
    "total = len(method_list)\n",
    "for i, (method_name, model) in enumerate(method_list, start=1):\n",
    "    print(f\"\\n[{i}/{total}] Training {method_name}...\")\n",
    "\n",
    "    model.hopt(x_train_scaled, y_train, param_grid=DEFAULT_PARAM_GRID, verbose=True)\n",
    "    model.fit(x_train_scaled, y_train)\n",
    "\n",
    "    print(f\"→ {method_name} training complete. Evaluating...\")\n",
    "\n",
    "    if TASK == \"regression\":\n",
    "        y_pred = model.predict(x_test_scaled)\n",
    "    elif TASK == \"classification\":\n",
    "        y_prob = model.predict(x_test_scaled)\n",
    "        y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "\n",
    "    acc = accuracy_metric(y_test, y_pred, task=TASK)\n",
    "    res_df.loc[method_name, \"ACC\"] = acc\n",
    "\n",
    "    print(f\"✓ {method_name} done — ACC: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MeanInstanceWrapperMLPNetworkRegressor</th>\n",
       "      <td>0.587974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanBagNetworkRegressor</th>\n",
       "      <td>0.577722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DynamicPoolingNetworkRegressor</th>\n",
       "      <td>0.574897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanBagWrapperMLPNetworkRegressor</th>\n",
       "      <td>0.552846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdditiveAttentionNetworkRegressor</th>\n",
       "      <td>0.522261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HopfieldAttentionNetworkRegressor</th>\n",
       "      <td>0.512950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanInstanceNetworkRegressor</th>\n",
       "      <td>0.510830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SelfAttentionNetworkRegressor</th>\n",
       "      <td>0.431029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ACC\n",
       "MeanInstanceWrapperMLPNetworkRegressor  0.587974\n",
       "MeanBagNetworkRegressor                 0.577722\n",
       "DynamicPoolingNetworkRegressor          0.574897\n",
       "MeanBagWrapperMLPNetworkRegressor       0.552846\n",
       "AdditiveAttentionNetworkRegressor       0.522261\n",
       "HopfieldAttentionNetworkRegressor       0.512950\n",
       "MeanInstanceNetworkRegressor            0.510830\n",
       "SelfAttentionNetworkRegressor           0.431029"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.sort_values(by=\"ACC\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qsarmil",
   "language": "python",
   "name": "qsarmil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
